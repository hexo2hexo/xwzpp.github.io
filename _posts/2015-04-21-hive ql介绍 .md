---
layout: post
title:   Hive QL 介绍
description: " Hive QL 介绍"
category: project
avatarimg: "/img/touxiang.jpg"
tags : [Hadoop,Hive]
duoshuo: true
---
主要对hive ql进行介绍。
<!-- more -->

##二、本节目标

本次讲解 Hive QL 分为以下几个要点：

> **1. 数据定义操作 - DDL**

> **2. 数据操作 - DML**

> **3. Hive QL 查询操作**


## 三、数据定义操作 - DDL

**（1）建表（CREATE）的语法如下：**


	CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name 
	  [(col_name data_type [COMMENT col_comment], ...)] 
	  [COMMENT table_comment] 
	  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 
	  [CLUSTERED BY (col_name, col_name, ...) 
	  [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 
	  [ROW FORMAT row_format] 
	  [STORED AS file_format] 
	  [LOCATION hdfs_path]
 

上面的一些关键字解释：

* **CREATE TABLE** 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXIST 选项来忽略这个异常
* **EXTERNAL** 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）
* **LIKE** 允许用户复制现有的表结构，但是不复制数据
* **COMMENT** 可以为表与字段增加描述
* **ROW FORMAT** 
> DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]
       用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表的具体的列的数据。
* **STORED AS**
            SEQUENCEFILE
            | TEXTFILE
            | RCFILE    
            | INPUTFORMAT input_format_classname OUTPUTFORMAT             output_format_classname
       如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCE 。

**（2）建表（CREATE）**

* 创建普通表
* 创建外部表
* 创建分区表
* 创建 Bucket 表


* 创建普通表
	
		CREATE TABLE page_view(viewTime INT, userid BIGINT,
		     page_url STRING, referrer_url STRING,
		     ip STRING COMMENT 'IP Address of the User',
		     country STRING COMMENT 'country of origination')
		 COMMENT 'This is the staging page view table';
	
 
* 创建外部表：


		CREATE EXTERNAL TABLE page_view(viewTime INT, userid BIGINT,
		     page_url STRING, referrer_url STRING,
		     ip STRING COMMENT 'IP Address of the User',
		     country STRING COMMENT 'country of origination')
		 COMMENT 'This is the staging page view table'
		 ROW FORMAT DELIMITED FIELDS TERMINATED BY '\054'
		 STORED AS TEXTFILE
		 LOCATION '<hdfs_location>';
	

* 创建分区表：


		CREATE TABLE par_table(viewTime INT, userid BIGINT,
		     page_url STRING, referrer_url STRING,
		     ip STRING COMMENT 'IP Address of the User')
		 COMMENT 'This is the page view table'
		 PARTITIONED BY(date STRING, pos STRING)
		ROW FORMAT DELIMITED ‘\t’
		   FIELDS TERMINATED BY '\n'
		STORED AS SEQUENCEFILE;
	

* 创建 Bucket 表：

		CREATE TABLE par_table(viewTime INT, userid BIGINT,
		     page_url STRING, referrer_url STRING,
		     ip STRING COMMENT 'IP Address of the User')
		 COMMENT 'This is the page view table'
		 PARTITIONED BY(date STRING, pos STRING)
		 CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS
		 ROW FORMAT DELIMITED ‘\t’
		   FIELDS TERMINATED BY '\n'
		STORED AS SEQUENCEFILE;
	


**（3）修改表结构**

 * 重命名表
* 增加、删除分区
* 增加、更新列
* 修改列的名字、类型、位置、注释
* 增加表的元数据信息
* ...


* 复制一个空表
	

		CREATE TABLE empty_key_value_store
		LIKE key_value_store;


* 删除表


		DROP TABLE table_name


* 重命名表

		
		ALTER TABLE table_name RENAME TO new_table_name 
		

* 增加、删除分区


		增加   
		ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION 'location1' ] partition_spec [ LOCATION 'location2' ] ...
		partition_spec:
		  : PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)
		
		删除   
		ALTER TABLE table_name DROP
		    partition_spec, partition_spec,...


* 增加、更新列

	
		ADD 是代表新增一字段，字段位置在所有列后面(partition列前)      
		REPLACE 则是表示替换表中所有字段。      
		ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)


* 修改列的名字、类型、位置、注释


		这个命令可以允许改变列名、数据类型、注释、列位置或者它们的任意组合      
		ALTER TABLE table_name CHANGE [COLUMN]
		col_old_name col_new_name column_type
		[COMMENT col_comment]
		[FIRST|AFTER column_name]


* 增加表的元数据信息

	
		用户可以用这个命令向表中增加元数据信息 metadata      
		ALTER TABLE table_name SET TBLPROPERTIES table_properties table_properties:
		    : (property_name = property_value, ...)


* 改变文件格式和组织


		ALTER TABLE table_name SET FILEFORMAT file_format
		ALTER TABLE table_name CLUSTERED BY(col_name, col_name, ...) 
		    [SORTED BY(col_name, ...)] INTO num_buckets BUCKETS


* 创建、删除视图


		# 创建视图
		CREATE VIEW [IF NOT EXISTS] view_name [ (column_name [COMMENT column_comment], ...) ][COMMENT view_comment][TBLPROPERTIES (property_name = property_value, ...)]
		AS SELECT ...
		
		# 删除视图
		DROP VIEW view_name


* 创建、删除函数
	
	
		# 创建函数
		CREATE TEMPORARY FUNCTION function_name AS class_name
		
		# 删除函数
		DROP TEMPORARY FUNCTION function_name
	

* 展示/描述 语句

		# 显示 表
		show tables;
		
		# 显示 数据库
		show databases;
		
		# 显示 分区
		show partitions;
		
		# 显示 函数
		show functions;
		
		# 描述 表/列
		describe [EXTENDED] table_name[DOT col_name]



## 四、数据操作 - DML

Hive 不支持使用 insert 语句一条一条的进行插入操作，也不支持 update 操作。数据是以 load 的方式加载到建立好的表中，数据一旦导入就不可以修改。

**（1）向数据表内加载文件**


	LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE]
	    INTO TABLE tablename
	    [PARTITION (partcol1=val1, partcol2=val2 ...)]


> * **Load** 

>   操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。

> * **filepath**
>
 * 相对路径，例如：project/data1
 * 绝对路径，例如： /user/hive/project/data1
 * 包含模式的完整 URI，例如：hdfs://namenode:9000/user/hive/project/data1

> * **LOCAL 关键字**
>
 * 指定了LOCAL
即本地 load 命令会去查找本地文件系统中的 filepath. 如果发现是相对路径，则路径会被解释为相对于当前用户的当前路径。用户也可以为本地文件指定一个完整的 URI，比如：file:///user/hive/project/data. 此时 load 命令会将 filepath 中的文件复制到目标文件系统中。目标文件系统由表的位置属性决定。被复制的数据文件移动到表的数据对应的位置。
 * 没有指定LOCAL
如果 filepath 指向的是一个完整的 URI，hive 会直接使用这个 URI. 否则如果没有指定 schema 或者 authority，Hive 会使用在 hadoop 配置文件中定义的 schema 和 authority，fs.default.name 指定了 Namenode 的 URI. 如果路径不是绝对的，Hive 相对于 /user/ 进行解释。 Hive 会将 filepath 中指定的文件内容移动到 table （或者 partition）所指定的路径中。

> * **OVERWRITE**

>  使用 OVERWRITE 关键字，目标表（或者分区）中的内容（如果有）会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。

例如：


	hive> LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes;


**（2）将查询结果插入Hive表**


	INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement


* 多插入模式
	

	FROM from_statement
	INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1
	[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...


* 自动分区模式


	INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement


**（3）将查询结果写入HDFS文件系统**


	INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ...
	FROM from_statement
	INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1
	[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ...


数据写入文件系统时会进行文本序列化，且每列用 `^A` 来区分，`\n` 换行。如果任何一列不是原始类型，那么这些将会被序列化为 JSON 格式。


## 五、Hive QL 查询操作

**SQL操作：**

> * 基本的 Select 操作
> * 基于 Partition 的查询
> * Join

**（1）基本的 Select 操作**


	SELECT [ALL | DISTINCT] select_expr, select_expr, ...
	FROM table_reference
	[WHERE where_condition]
	[GROUP BY col_list]
	[   CLUSTER BY col_list
	  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]
	]
	[LIMIT number]


> 使用 **ALL** 和 **DISTINCT** 选项区分对重复记录的处理。默认是 ALL，表示查询所有记录。DISTINCT 表示去掉重复的记录；

> **Limit** 可以限制查询输出的记录数；

**（2）基于 Partition 的查询**

一般 SELECT 查询会扫描整个表，使用 PARTITIONED BY 子句建表，查询就可以利用分区剪枝（input pruning）的特性。

Hive 当前的分区剪枝，只有分区断言出现在离 FROM 子句最近的那个 WHERE 子句中，才会启用分区剪枝。

例如，一个表 page_view 按照 date 列的值进行了分区，那么下面的查询就可以检索出日期为 2010-03-01 的行记录：


	SELECT page_view
	    FROM page_view
	    WHERE page_view.date >= '2010-03-01' ADN page_views.date <= '2010-03-31'


**（3）Join**

Join 的语法如下：
	

	join_table:
	    table_reference JOIN table_factor [join_condition]
	  | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition
	  | table_reference LEFT SEMI JOIN table_reference join_condition
	  | table_reference CROSS JOIN table_reference [join_condition] (as of Hive 0.10)
	
	table_reference:
	    table_factor
	  | join_table
	
	table_factor:
	    tbl_name [alias]
	  | table_subquery alias
	  | ( table_references )
	
	join_condition:
	    ON equality_expression ( AND equality_expression )*
	
	equality_expression:
	    expression = expression


> hive 只支持等连接（equality joins）、外连接（outer joins）、左半连接（left semi joins）。hive 不支持非相等的 join 条件，因为它很难在 map/reduce job 中实现这样的条件。而且，hive 可以 join 两个以上的表。
